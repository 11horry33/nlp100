{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **80. ID番号への変換**"
      ],
      "metadata": {
        "id": "2XrzsDSF8uuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvoHb-BFhkA2",
        "outputId": "ab558ff7-d39f-44b6-ffc6-e10155fa14df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-qZGlC38qdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63bb143-43a1-43c5-9337-3e789de7dd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-22 08:14:57--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘NewsAggregatorDataset.zip’\n",
            "\n",
            "NewsAggregatorDatas     [     <=>            ]  27.87M  31.5MB/s    in 0.9s    \n",
            "\n",
            "2023-06-22 08:14:59 (31.5 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203]\n",
            "\n",
            "Archive:  NewsAggregatorDataset.zip\n",
            "  inflating: 2pageSessions.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "  inflating: newsCorpora.csv         \n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ]
        }
      ],
      "source": [
        "# データダウンロード\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読み込み\n",
        "df = pd.read_csv(\"newsCorpora.csv\", sep=\"\\t\", names=(\"ID\",\"TITLE\",\"URL\",\"PUBLISHER\",\"CATEGORY\",\"STORY\",\"HOSTNAME\",\"TIMESTAMP\"))\n",
        "\n",
        "# \"TITLE\"と\"CATEGORY\"を抽出\n",
        "data = df.loc[df[\"PUBLISHER\"].isin([\"Reuters\",\"Huffington Post\",\"Businessweek\",\"Contactmusic.com\",\"Daily Mail\"]), [\"TITLE\",\"CATEGORY\"]]\n",
        "\n",
        "# データ分割　学習:検証:テスト=8:1:1\n",
        "train, others = train_test_split(data, test_size=0.2, random_state=0, shuffle=True)\n",
        "dev, test = train_test_split(others, test_size=0.5, random_state=0, shuffle=True)\n",
        "\n",
        "# ファイルに保存\n",
        "train.to_csv(\"train.txt\", sep=\"\\t\", index=None)\n",
        "dev.to_csv(\"dev.txt\", sep=\"\\t\", index=None)\n",
        "test.to_csv(\"test.txt\", sep=\"\\t\", index=None)\n",
        "\n",
        "# 事例数の確認\n",
        "print(f'学習データの事例数\\n{train[\"CATEGORY\"].value_counts()}\\n')\n",
        "print(f'検証データの事例数\\n{dev[\"CATEGORY\"].value_counts()}\\n')\n",
        "print(f'テストデータの事例数\\n{test[\"CATEGORY\"].value_counts()}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0taZdSqER701",
        "outputId": "66735a2d-bde5-49a3-9e00-9a5f293409b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習データの事例数\n",
            "b    4481\n",
            "e    4240\n",
            "t    1214\n",
            "m     737\n",
            "Name: CATEGORY, dtype: int64\n",
            "\n",
            "検証データの事例数\n",
            "b    575\n",
            "e    528\n",
            "t    137\n",
            "m     94\n",
            "Name: CATEGORY, dtype: int64\n",
            "\n",
            "テストデータの事例数\n",
            "b    571\n",
            "e    511\n",
            "t    173\n",
            "m     79\n",
            "Name: CATEGORY, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def preprocessing(text):\n",
        "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    text = text.translate(table)  # 記号をスペースに置換\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # 2つ以上の空白を1つにまとめる\n",
        "    return text"
      ],
      "metadata": {
        "id": "tbZtVf8ATjCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# 前処理\n",
        "train[\"TITLE\"] = train[\"TITLE\"].map(lambda x: preprocessing(x))\n",
        "\n",
        "# 全文章をまとめる\n",
        "all_sentences = \" \".join(train[\"TITLE\"].tolist()).split(\" \")\n",
        "\n",
        "# 単語の頻度を計算\n",
        "all_word_cnt = collections.Counter(all_sentences) # Counterは辞書型\n",
        "del all_word_cnt[\"\"] # 空白を削除\n",
        "\n",
        "# IDの付与\n",
        "word2id = {word: i + 1 for i, (word, cnt) in enumerate(all_word_cnt.most_common()) if cnt > 1} # most_common()で（単語, 頻度）形式のリストにする\n",
        "\n",
        "print(f'ID数: {len(set(word2id.values()))}\\n')\n",
        "print('頻度上位12語')\n",
        "for key in list(word2id)[:12]:\n",
        "    print(f'{key}: {word2id[key]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkZ3QUn_U_hq",
        "outputId": "049d4d19-8768-44b2-8cfa-2bca97fd2e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID数: 9481\n",
            "\n",
            "頻度上位12語\n",
            "to: 1\n",
            "s: 2\n",
            "in: 3\n",
            "UPDATE: 4\n",
            "on: 5\n",
            "as: 6\n",
            "US: 7\n",
            "for: 8\n",
            "of: 9\n",
            "The: 10\n",
            "1: 11\n",
            "To: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語に対応するIDを返す関数：未知語は\"0\"を返す\n",
        "def word_to_id(text, word2id=word2id, unk=0):\n",
        "  return [word2id.get(word, unk) for word in text.split()]"
      ],
      "metadata": {
        "id": "4J4LEBa9ZDyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I have a pen .\"\n",
        "print(f'テキスト: {text}')\n",
        "print(f'ID列: {word_to_id(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TusdB2b7awpX",
        "outputId": "de97304a-bccb-439f-c37e-432691a6aab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "テキスト: I have a pen .\n",
            "ID列: [84, 209, 19, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **81. RNNによる予測**"
      ],
      "metadata": {
        "id": "_FbqcC_-cNKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, X, y, word_to_id):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.word_to_id = word_to_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.X[index]\n",
        "        inputs = self.word_to_id(text)\n",
        "\n",
        "        return {\n",
        "          'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "          'labels': torch.tensor(self.y[index], dtype=torch.int64)\n",
        "        }"
      ],
      "metadata": {
        "id": "t00dSig2cRMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "\n",
        "# train\n",
        "df_train = pd.read_csv(\"train.txt\", sep=\"\\t\") # 読み込み\n",
        "X_train = df_train[\"TITLE\"].map(preprocessing) # 前処理\n",
        "y_train = df_train[\"CATEGORY\"].map(lambda x: category_dict[x]).values # ラベルを数値に変換\n",
        "\n",
        "# dev\n",
        "df_dev = pd.read_csv(\"dev.txt\", sep=\"\\t\")\n",
        "X_dev = df_dev[\"TITLE\"].map(preprocessing)\n",
        "y_dev = df_dev[\"CATEGORY\"].map(lambda x: category_dict[x]).values\n",
        "\n",
        "# test\n",
        "df_test = pd.read_csv(\"test.txt\", sep=\"\\t\")\n",
        "X_test = df_test[\"TITLE\"].map(preprocessing)\n",
        "y_test = df_test[\"CATEGORY\"].map(lambda x: category_dict[x]).values\n",
        "\n",
        "# データセット作成\n",
        "dataset_train = CreateDataset(X_train, y_train, word_to_id)\n",
        "dataset_dev = CreateDataset(X_dev, y_dev, word_to_id)\n",
        "dataset_test = CreateDataset(X_test, y_test, word_to_id)"
      ],
      "metadata": {
        "id": "2_p2HS8d77c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, n_input, n_embed, n_hidden, n_output, padding_idx):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=n_input, embedding_dim=n_embed, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(input_size=n_embed, hidden_size=n_hidden)\n",
        "        self.fc = nn.Linear(in_features=n_hidden, out_features=n_output)\n",
        "\n",
        "    # 順伝播\n",
        "    def forward(self, x):\n",
        "        o, (h, c) = self.lstm(self.embed(x))\n",
        "        return self.fc(o[:, -1, :])"
      ],
      "metadata": {
        "id": "XzPEn2fchGFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = 128  # 単語ベクトルの次元\n",
        "n_hidden = 256  # 文ベクトルの次元\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "padding_idx = len(set(word2id.values()))\n",
        "\n",
        "# モデル定義\n",
        "model = RNN(n_input, n_embed, n_hidden, n_output, padding_idx)\n",
        "\n",
        "# 先頭10件の予測値取得\n",
        "for i in range(10):\n",
        "    X = dataset_train[i][\"inputs\"]\n",
        "    print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evSFGYzvB1Sp",
        "outputId": "3996ab70-141e-4b9c-d1f7-a3054346aa34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2509, 0.2519, 0.2480, 0.2492]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2580, 0.2427, 0.2397, 0.2596]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2541, 0.2288, 0.2535, 0.2636]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2650, 0.2392, 0.2469, 0.2490]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2607, 0.2480, 0.2464, 0.2449]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2328, 0.2487, 0.2541, 0.2644]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2488, 0.2410, 0.2566, 0.2536]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2515, 0.2536, 0.2391, 0.2558]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2696, 0.2338, 0.2542, 0.2423]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2568, 0.2406, 0.2590, 0.2436]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **82. 確率的勾配降下法による学習**"
      ],
      "metadata": {
        "id": "G00Yk2ydH-C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "# データセットオブジェクトからデータローダーを作成\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dataset_dev, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "IuaTl3usIDQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_accuracy(model, loader, criterion, device):\n",
        "    with torch.no_grad():\n",
        "        acc_list = []\n",
        "        loss_list = []\n",
        "        for batch in loader:\n",
        "            x = batch[\"inputs\"].to(device)\n",
        "            t = batch[\"labels\"].to(device)\n",
        "            y = model(x)\n",
        "\n",
        "            # loss計算\n",
        "            loss = criterion(y, t).item()\n",
        "            loss_list.append(loss)\n",
        "\n",
        "            # accuracy計算\n",
        "            pred = torch.argmax(y, dim=1)\n",
        "            acc = (pred == t).sum().item() * 1.0 / len(t)\n",
        "            acc_list.append(acc)\n",
        "\n",
        "    return torch.tensor(loss_list).mean(), torch.tensor(acc_list).mean()\n",
        "\n",
        "\n",
        "def train_model(batch_size, model, criterion, optimizer, num_epochs, nlp, device=None):\n",
        "    model.to(device)\n",
        "\n",
        "    log_train, log_dev = [], []\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x = batch[\"inputs\"].to(device)\n",
        "            t = batch[\"labels\"].to(device)\n",
        "            y = model(x)\n",
        "            loss = criterion(y, t)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 推論モード\n",
        "        model.eval()\n",
        "\n",
        "        loss_train, acc_train = calc_loss_accuracy(model, train_loader, criterion, device)\n",
        "        loss_dev, acc_dev = calc_loss_accuracy(model, dev_loader, criterion, device)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_dev.append([loss_dev, acc_dev])\n",
        "\n",
        "        # チェックポイント保存\n",
        "        torch.save({\"epoch\":i, \"model_state_dict\":model.state_dict(), \"optimizer_state_dict\":optimizer.state_dict()}, f\"{nlp}/checkpoint_{i+1}.pt\")\n",
        "\n",
        "        # ログ表示\n",
        "        print(f\"epoch:{i+1}  loss_train:{loss_train:.4f}  acc_train:{acc_train:.4f}  loss_dev:{loss_dev:.4f}  acc_dev:{acc_dev:.4f}\")"
      ],
      "metadata": {
        "id": "sq8j2J5aI6NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir nlp82\n",
        "\n",
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = 128  # 単語ベクトルの次元\n",
        "n_hidden = 256  # 文ベクトルの次元\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "padding_idx = len(set(word2id.values()))\n",
        "nlp=\"nlp82\"\n",
        "\n",
        "# モデル定義\n",
        "model = RNN(n_input, n_embed, n_hidden, n_output, padding_idx)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #損失関数\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) # 最適化手法\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "train_model(1, model, criterion, optimizer, 5, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_JRP3QcKS2w",
        "outputId": "b5753e0f-7205-4c4e-dbe0-eeaa8a23ff2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘nlp82’: File exists\n",
            "epoch:1  loss_train:0.9891  acc_train:0.5970  loss_dev:1.1112  acc_dev:0.5210\n",
            "epoch:2  loss_train:0.7810  acc_train:0.7115  loss_dev:1.0037  acc_dev:0.6064\n",
            "epoch:3  loss_train:0.6669  acc_train:0.7543  loss_dev:0.9617  acc_dev:0.6522\n",
            "epoch:4  loss_train:0.5875  acc_train:0.7759  loss_dev:0.9925  acc_dev:0.6514\n",
            "epoch:5  loss_train:0.5661  acc_train:0.7852  loss_dev:1.0197  acc_dev:0.6327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **83. ミニバッチ化・GPU上での学習**"
      ],
      "metadata": {
        "id": "_mx_ubvLQ3fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, n_input, n_embed, n_hidden, n_output, padding_idx):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=n_input, embedding_dim=n_embed, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(input_size=n_embed, hidden_size=n_hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=n_hidden, out_features=n_output)\n",
        "\n",
        "    # 順伝播\n",
        "    def forward(self, x):\n",
        "        o, (h, c) = self.lstm(self.embed(x))\n",
        "        return self.fc(o[:, -1, :])"
      ],
      "metadata": {
        "id": "Po7s73Q-f_e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 最大系列長に揃える関数を定義：ミニバッチでは系列長を揃えないとダメ\n",
        "def collate_fn_(batch):\n",
        "    sequences = [x[\"inputs\"] for x in batch]\n",
        "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=len(set(word2id.values()))) # パディング\n",
        "    labels = torch.LongTensor([x[\"labels\"] for x in batch])\n",
        "\n",
        "    return {'inputs': sequences_padded, 'labels': labels}\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# データセットオブジェクトからデータローダーを作成\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_)\n",
        "dev_loader = DataLoader(dataset_dev, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_)\n",
        "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_)"
      ],
      "metadata": {
        "id": "wgNiQ5LqRA4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir nlp83\n",
        "\n",
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = 128  # 単語ベクトルの次元\n",
        "n_hidden = 256  # 文ベクトルの次元\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "padding_idx = len(set(word2id.values()))\n",
        "nlp=\"nlp83\"\n",
        "\n",
        "# モデル定義\n",
        "model = RNN(n_input, n_embed, n_hidden, n_output, padding_idx)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #損失関数\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05) # 最適化手法\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_model(32, model, criterion, optimizer, 10, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bS5ynyXUeUH",
        "outputId": "0c4843ad-89db-4485-f912-67221974f2c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘nlp83’: File exists\n",
            "RNN(\n",
            "  (embed): Embedding(9482, 128, padding_idx=9481)\n",
            "  (lstm): LSTM(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "epoch:1  loss_train:1.1583  acc_train:0.4372  loss_dev:1.1438  acc_dev:0.4571\n",
            "epoch:2  loss_train:1.1484  acc_train:0.4908  loss_dev:1.1360  acc_dev:0.4910\n",
            "epoch:3  loss_train:1.1224  acc_train:0.5262  loss_dev:1.1095  acc_dev:0.5275\n",
            "epoch:4  loss_train:0.9568  acc_train:0.6412  loss_dev:0.9228  acc_dev:0.6569\n",
            "epoch:5  loss_train:0.9790  acc_train:0.6445  loss_dev:0.9642  acc_dev:0.6403\n",
            "epoch:6  loss_train:0.7832  acc_train:0.7275  loss_dev:0.7598  acc_dev:0.7405\n",
            "epoch:7  loss_train:0.7164  acc_train:0.7505  loss_dev:0.7033  acc_dev:0.7635\n",
            "epoch:8  loss_train:0.6332  acc_train:0.7747  loss_dev:0.6601  acc_dev:0.7754\n",
            "epoch:9  loss_train:0.6785  acc_train:0.7599  loss_dev:0.7479  acc_dev:0.7391\n",
            "epoch:10  loss_train:0.5530  acc_train:0.7905  loss_dev:0.6682  acc_dev:0.7597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **84. 単語ベクトルの導入**"
      ],
      "metadata": {
        "id": "GZYO6Z6wb0y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリのインストール\n",
        "! pip install gensim==4.0.1\n",
        "\n",
        "from gensim import models\n",
        "\n",
        "# 単語ベクトルの読み込み\n",
        "en_w2v = models.KeyedVectors.load_word2vec_format('drive/MyDrive/nlp100/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "wviI1tqkcKNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff65ff1-e125-498f-c319-4d95f5a48302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==4.0.1\n",
            "  Downloading gensim-4.0.1.tar.gz (23.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.10.1)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (6.3.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
            "Failed to build gensim\n",
            "\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 学習済み単語ベクトルの取得\n",
        "n_inputs = len(set(word2id.values())) + 1\n",
        "n_embed = 300\n",
        "weights = np.zeros((n_inputs, n_embed))\n",
        "words_in_pretrained = 0\n",
        "for i, word in enumerate(word2id.keys()):\n",
        "  try:\n",
        "    weights[i] = en_w2v[word]\n",
        "    words_in_pretrained += 1\n",
        "  except KeyError:\n",
        "    weights[i] = np.random.normal(scale=0.4, size=(n_embed,))\n",
        "weights = torch.from_numpy(weights.astype((np.float32)))\n",
        "\n",
        "print(f'学習済みベクトル利用単語数: {words_in_pretrained} / {n_inputs}')\n",
        "print(weights.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ibHpq14iIf9",
        "outputId": "ddc65d2f-cb41-4758-e28f-d7a36b2c8276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習済みベクトル利用単語数: 9239 / 9482\n",
            "torch.Size([9482, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, n_input, n_embed, n_hidden, n_output, padding_idx, emb_weights=None):\n",
        "        super(RNN, self).__init__()\n",
        "        if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n",
        "            self.embed = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "        else:\n",
        "            self.embed = nn.Embedding(num_embeddings=n_input, embedding_dim=n_embed, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(input_size=n_embed, hidden_size=n_hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=n_hidden, out_features=n_output)\n",
        "\n",
        "    # 順伝播\n",
        "    def forward(self, x):\n",
        "        o, (h, c) = self.lstm(self.embed(x))\n",
        "        return self.fc(o[:, -1, :])"
      ],
      "metadata": {
        "id": "JMJFexwfmH1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir nlp84\n",
        "\n",
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = 300  # 単語ベクトルの次元\n",
        "n_hidden = 256  # 文ベクトルの次元\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "padding_idx = len(set(word2id.values()))\n",
        "nlp=\"nlp84\"\n",
        "\n",
        "# モデル定義\n",
        "model = RNN(n_input, n_embed, n_hidden, n_output, padding_idx, emb_weights=weights)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #損失関数\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05) # 最適化手法\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_model(32, model, criterion, optimizer, 10, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lFIA4Hjm2mF",
        "outputId": "27ff9780-3d60-45e7-d76e-0ddd52e1fa1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘nlp84’: File exists\n",
            "RNN(\n",
            "  (embed): Embedding(9482, 300, padding_idx=9481)\n",
            "  (lstm): LSTM(300, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "epoch:1  loss_train:1.1625  acc_train:0.4228  loss_dev:1.1483  acc_dev:0.4355\n",
            "epoch:2  loss_train:1.1595  acc_train:0.4389  loss_dev:1.1459  acc_dev:0.4531\n",
            "epoch:3  loss_train:1.1561  acc_train:0.4683  loss_dev:1.1435  acc_dev:0.4788\n",
            "epoch:4  loss_train:1.1514  acc_train:0.4705  loss_dev:1.1395  acc_dev:0.4832\n",
            "epoch:5  loss_train:1.1446  acc_train:0.4896  loss_dev:1.1336  acc_dev:0.5003\n",
            "epoch:6  loss_train:1.1372  acc_train:0.4869  loss_dev:1.1225  acc_dev:0.5007\n",
            "epoch:7  loss_train:1.1275  acc_train:0.4993  loss_dev:1.0971  acc_dev:0.5078\n",
            "epoch:8  loss_train:1.0518  acc_train:0.5753  loss_dev:1.0595  acc_dev:0.5752\n",
            "epoch:9  loss_train:0.9680  acc_train:0.6488  loss_dev:0.9452  acc_dev:0.6506\n",
            "epoch:10  loss_train:0.9367  acc_train:0.6589  loss_dev:0.9030  acc_dev:0.6763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **85. 双方向・多層化**"
      ],
      "metadata": {
        "id": "0tzco6mDoZcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, n_input, n_embed, n_hidden, n_layers, n_output, padding_idx, bidirectional, emb_weights=None):\n",
        "        super(RNN, self).__init__()\n",
        "        if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n",
        "            self.embed = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "        else:\n",
        "            self.embed = nn.Embedding(num_embeddings=n_input, embedding_dim=n_embed, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(input_size=n_embed, hidden_size=n_hidden, num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=n_hidden * (2 if bidirectional==True else 1), out_features=n_output)\n",
        "\n",
        "    # 順伝播\n",
        "    def forward(self, x):\n",
        "        o, (h, c) = self.lstm(self.embed(x))\n",
        "        return self.fc(o[:, -1, :])"
      ],
      "metadata": {
        "id": "JGGntXx8oelr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir nlp85\n",
        "\n",
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = 300  # 単語ベクトルの次元\n",
        "n_hidden = 256  # 文ベクトルの次元\n",
        "n_layers = 2\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数\n",
        "bidirectional = True\n",
        "padding_idx = len(set(word2id.values()))\n",
        "nlp=\"nlp85\"\n",
        "\n",
        "# モデル定義\n",
        "model = RNN(n_input, n_embed, n_hidden, n_layers, n_output, padding_idx, bidirectional, emb_weights=weights)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #損失関数\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05) # 最適化手法\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_model(32, model, criterion, optimizer, 10, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdlilhKpo16o",
        "outputId": "9929e929-437c-49ae-c50e-2183f60a3548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (embed): Embedding(9482, 300, padding_idx=9481)\n",
            "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n",
            "epoch:1  loss_train:1.1624  acc_train:0.4200  loss_dev:1.1483  acc_dev:0.4326\n",
            "epoch:2  loss_train:1.1621  acc_train:0.3973  loss_dev:1.1492  acc_dev:0.3949\n",
            "epoch:3  loss_train:1.1604  acc_train:0.4673  loss_dev:1.1466  acc_dev:0.4958\n",
            "epoch:4  loss_train:1.1578  acc_train:0.4583  loss_dev:1.1430  acc_dev:0.4784\n",
            "epoch:5  loss_train:1.1542  acc_train:0.4682  loss_dev:1.1376  acc_dev:0.4899\n",
            "epoch:6  loss_train:1.1456  acc_train:0.4898  loss_dev:1.1304  acc_dev:0.5122\n",
            "epoch:7  loss_train:1.1347  acc_train:0.4894  loss_dev:1.1178  acc_dev:0.5085\n",
            "epoch:8  loss_train:1.1128  acc_train:0.5256  loss_dev:1.0983  acc_dev:0.5272\n",
            "epoch:9  loss_train:1.0145  acc_train:0.6145  loss_dev:0.9881  acc_dev:0.6221\n",
            "epoch:10  loss_train:0.9698  acc_train:0.6424  loss_dev:0.9519  acc_dev:0.6418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **86. 畳込みニューラルネットワーク（CNN）**"
      ],
      "metadata": {
        "id": "2M80ReC8x_AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 最大系列長に揃える関数を定義：ミニバッチでは系列長を揃えないとダメ\n",
        "def collate_fn_(batch):\n",
        "    sequences = [x[\"inputs\"] for x in batch]\n",
        "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=len(set(word2id.values()))) # パディング\n",
        "    labels = torch.LongTensor([x[\"labels\"] for x in batch])\n",
        "\n",
        "    return {'inputs': sequences_padded, 'labels': labels}\n",
        "\n",
        "\n",
        "def calc_loss_accuracy(model, loader, criterion, device):\n",
        "    with torch.no_grad():\n",
        "        acc_list = []\n",
        "        loss_list = []\n",
        "        for batch in loader:\n",
        "            x = batch[\"inputs\"].to(device)\n",
        "            t = batch[\"labels\"].to(device)\n",
        "            y = model(x)\n",
        "\n",
        "            # loss計算\n",
        "            loss = criterion(y, t).item()\n",
        "            loss_list.append(loss)\n",
        "\n",
        "            # accuracy計算\n",
        "            pred = torch.argmax(y, dim=1)\n",
        "            acc = (pred == t).sum().item() * 1.0 / len(t)\n",
        "            acc_list.append(acc)\n",
        "\n",
        "    return torch.tensor(loss_list).mean(), torch.tensor(acc_list).mean()\n",
        "\n",
        "\n",
        "def train_model(batch_size, model, criterion, optimizer, num_epochs, nlp, device=None):\n",
        "    model.to(device)\n",
        "\n",
        "    log_train, log_dev = [], []\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x = batch[\"inputs\"].to(device)\n",
        "            t = batch[\"labels\"].to(device)\n",
        "            y = model(x)\n",
        "            loss = criterion(y, t)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 推論モード\n",
        "        model.eval()\n",
        "\n",
        "        loss_train, acc_train = calc_loss_accuracy(model, train_loader, criterion, device)\n",
        "        loss_dev, acc_dev = calc_loss_accuracy(model, dev_loader, criterion, device)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_dev.append([loss_dev, acc_dev])\n",
        "\n",
        "        # チェックポイント保存\n",
        "        torch.save({\"epoch\":i, \"model_state_dict\":model.state_dict(), \"optimizer_state_dict\":optimizer.state_dict()}, f\"{nlp}/checkpoint_{i+1}.pt\")\n",
        "\n",
        "        # ログ表示\n",
        "        print(f\"epoch:{i+1}  loss_train:{loss_train:.4f}  acc_train:{acc_train:.4f}  loss_dev:{loss_dev:.4f}  acc_dev:{acc_dev:.4f}\")"
      ],
      "metadata": {
        "id": "2aIRGhIjyJZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_input, n_embed, n_output, padding_idx, out_channels, kernel_size, stride, padding):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=n_input, embedding_dim=n_embed, padding_idx=padding_idx)\n",
        "        self.conv = nn.Conv2d(1, out_channels, (kernel_size, n_embed), stride, (padding, 0))\n",
        "        self.fc = nn.Linear(in_features=out_channels, out_features=n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.embed(x)                   # [1,len(x),embed_size]\n",
        "        h = self.conv(h.unsqueeze(1))    # [batch_size,out_channnels,len(x),1]\n",
        "        h = F.relu(h.squeeze(3))          # [batch_size,out_channnels,len(x)]\n",
        "        h = F.max_pool1d(h, h.size()[2])  # [batch_size,out_channnels,1]\n",
        "        y = self.fc(h.squeeze(2))\n",
        "        return y"
      ],
      "metadata": {
        "id": "Enoqaf_cLWgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = 300  # 単語ベクトルの次元\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "padding_idx = len(set(word2id.values()))\n",
        "out_channels = 100\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1\n",
        "\n",
        "model = CNN(n_input, n_embed, n_output, padding_idx, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "# 先頭10件の予測値取得\n",
        "for i in range(10):\n",
        "    X = dataset_train[i][\"inputs\"]\n",
        "    print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DL1Khx6OhyG",
        "outputId": "3d6dc524-1e77-4f39-e3ca-d702af930528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2070, 0.1492, 0.2939, 0.3499]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2277, 0.1267, 0.3018, 0.3438]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2528, 0.1360, 0.3007, 0.3105]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2406, 0.1736, 0.2862, 0.2997]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1836, 0.1954, 0.3485, 0.2725]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1974, 0.2155, 0.2973, 0.2898]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2021, 0.1909, 0.2508, 0.3561]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2314, 0.2127, 0.2668, 0.2892]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1971, 0.2304, 0.2500, 0.3226]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2175, 0.1357, 0.2372, 0.4095]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **87. 確率的勾配降下法によるCNNの学習**"
      ],
      "metadata": {
        "id": "73AqqKd5P1GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "# データセットオブジェクトからデータローダーを作成\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_)\n",
        "dev_loader = DataLoader(dataset_dev, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_)\n",
        "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_)\n",
        "\n",
        "! mkdir nlp87\n",
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = 300  # 単語ベクトルの次元\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "padding_idx = len(set(word2id.values()))\n",
        "out_channels = 100\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1\n",
        "nlp = \"nlp87\"\n",
        "\n",
        "model = CNN(n_input, n_embed, n_output, padding_idx, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #損失関数\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05) # 最適化手法\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_model(32, model, criterion, optimizer, 10, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpuvOe-cP5Q6",
        "outputId": "98d1c9ed-0982-4dd7-f846-1bce3b355e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘nlp87’: File exists\n",
            "epoch:1  loss_train:0.3798  acc_train:0.8754  loss_dev:0.5309  acc_dev:0.8107\n",
            "epoch:2  loss_train:0.1452  acc_train:0.9640  loss_dev:0.4315  acc_dev:0.8561\n",
            "epoch:3  loss_train:0.0645  acc_train:0.9946  loss_dev:0.4411  acc_dev:0.8483\n",
            "epoch:4  loss_train:0.0259  acc_train:0.9987  loss_dev:0.4231  acc_dev:0.8580\n",
            "epoch:5  loss_train:0.0192  acc_train:0.9989  loss_dev:0.4437  acc_dev:0.8575\n",
            "epoch:6  loss_train:0.0169  acc_train:0.9988  loss_dev:0.4347  acc_dev:0.8650\n",
            "epoch:7  loss_train:0.0148  acc_train:0.9989  loss_dev:0.4421  acc_dev:0.8650\n",
            "epoch:8  loss_train:0.0161  acc_train:0.9988  loss_dev:0.4587  acc_dev:0.8594\n",
            "epoch:9  loss_train:0.0122  acc_train:0.9990  loss_dev:0.4511  acc_dev:0.8657\n",
            "epoch:10  loss_train:0.0121  acc_train:0.9990  loss_dev:0.4548  acc_dev:0.8665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **88. パラメータチューニング**"
      ],
      "metadata": {
        "id": "dkp0FLJMTe7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV3wWWWsTjf3",
        "outputId": "83734fc3-d467-4a00-f06d-9255fba79e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_nosave(batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "    model.to(device)\n",
        "\n",
        "    log_train, log_dev = [], []\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x = batch[\"inputs\"].to(device)\n",
        "            t = batch[\"labels\"].to(device)\n",
        "            y = model(x)\n",
        "            loss = criterion(y, t)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 推論モード\n",
        "        model.eval()\n",
        "\n",
        "        loss_train, acc_train = calc_loss_accuracy(model, train_loader, criterion, device)\n",
        "        loss_dev, acc_dev = calc_loss_accuracy(model, dev_loader, criterion, device)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_dev.append([loss_dev, acc_dev])\n",
        "\n",
        "        # ログ表示\n",
        "        print(f\"epoch:{i+1}  loss_train:{loss_train:.4f}  acc_train:{acc_train:.4f}  loss_dev:{loss_dev:.4f}  acc_dev:{acc_dev:.4f}\")"
      ],
      "metadata": {
        "id": "AvyAJMw4WMfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_input, n_embed, n_output, padding_idx, out_channels, kernel_size, stride, padding, dropout_rate):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=n_input, embedding_dim=n_embed, padding_idx=padding_idx)\n",
        "        self.conv = nn.Conv2d(1, out_channels, (kernel_size, n_embed), stride, (padding, 0))\n",
        "        self.fc = nn.Linear(in_features=out_channels, out_features=n_output)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.embed(x)                   # [1,len(x),embed_size]\n",
        "        h = self.conv(h.unsqueeze(1))    # [batch_size,out_channnels,len(x),1]\n",
        "        h = F.relu(h.squeeze(3))          # [batch_size,out_channnels,len(x)]\n",
        "        h = F.max_pool1d(h, h.size()[2])  # [batch_size,out_channnels,1]\n",
        "        y = self.fc(self.dropout(h.squeeze(2)))\n",
        "        return y"
      ],
      "metadata": {
        "id": "3AOh73rDT9YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # チューニング対象パラメータのセット\n",
        "    emb_size = int(trial.suggest_discrete_uniform('emb_size', 100, 400, 100))\n",
        "    out_channels = int(trial.suggest_discrete_uniform('out_channels', 50, 200, 50))\n",
        "    drop_rate = trial.suggest_discrete_uniform('drop_rate', 0.0, 0.5, 0.1)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 5e-4, 5e-2)\n",
        "    momentum = trial.suggest_discrete_uniform('momentum', 0.5, 0.9, 0.1)\n",
        "\n",
        "    # 固定\n",
        "    n_input = len(set(word2id.values())) + 1\n",
        "    n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "    padding_idx = len(set(word2id.values()))\n",
        "    kernel_size = 3\n",
        "    stride = 1\n",
        "    padding = 1\n",
        "\n",
        "    model = CNN(n_input, n_embed, n_output, padding_idx, out_channels, kernel_size, stride, padding, drop_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() #損失関数\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) # 最適化手法\n",
        "\n",
        "    device = torch.cuda.set_device(0)\n",
        "\n",
        "    train_model_nosave(32, model, criterion, optimizer, 5)\n",
        "\n",
        "    _, dev_acc = calc_loss_accuracy(model, dev_loader, criterion, device)\n",
        "\n",
        "    return dev_acc"
      ],
      "metadata": {
        "id": "Ki4iELZDToLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, timeout=1200)\n",
        "\n",
        "# 結果の表示\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: {:.3f}'.format(trial.value))\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "  print('    {}: {}'.format(key, value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G6G8oloWwYj",
        "outputId": "18c9a6d3-cb6a-4913-dbe0-c85012c4c00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:36:19,569] A new study created in memory with name: no-name-bf49bab1-9b05-4fb7-8f05-0217fba0c903\n",
            "<ipython-input-34-d71b82b34403>:5: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
            "  emb_size = int(trial.suggest_discrete_uniform('emb_size', 100, 400, 100))\n",
            "<ipython-input-34-d71b82b34403>:6: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
            "  out_channels = int(trial.suggest_discrete_uniform('out_channels', 50, 200, 50))\n",
            "<ipython-input-34-d71b82b34403>:7: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
            "  drop_rate = trial.suggest_discrete_uniform('drop_rate', 0.0, 0.5, 0.1)\n",
            "<ipython-input-34-d71b82b34403>:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-4, 5e-2)\n",
            "<ipython-input-34-d71b82b34403>:9: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
            "  momentum = trial.suggest_discrete_uniform('momentum', 0.5, 0.9, 0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.5040  acc_train:0.8280  loss_dev:0.5797  acc_dev:0.8111\n",
            "epoch:2  loss_train:0.1446  acc_train:0.9654  loss_dev:0.4269  acc_dev:0.8583\n",
            "epoch:3  loss_train:0.0429  acc_train:0.9949  loss_dev:0.4272  acc_dev:0.8584\n",
            "epoch:4  loss_train:0.0213  acc_train:0.9985  loss_dev:0.4140  acc_dev:0.8695\n",
            "epoch:5  loss_train:0.0154  acc_train:0.9988  loss_dev:0.4247  acc_dev:0.8684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:37:05,246] Trial 0 finished with value: 0.8683712482452393 and parameters: {'emb_size': 300.0, 'out_channels': 100.0, 'drop_rate': 0.4, 'learning_rate': 0.006774270419053273, 'momentum': 0.9}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.5821  acc_train:0.8450  loss_dev:0.7283  acc_dev:0.7543\n",
            "epoch:2  loss_train:0.1394  acc_train:0.9699  loss_dev:0.4279  acc_dev:0.8517\n",
            "epoch:3  loss_train:0.0530  acc_train:0.9967  loss_dev:0.4585  acc_dev:0.8331\n",
            "epoch:4  loss_train:0.0220  acc_train:0.9987  loss_dev:0.4136  acc_dev:0.8621\n",
            "epoch:5  loss_train:0.0165  acc_train:0.9989  loss_dev:0.4205  acc_dev:0.8658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:38:16,729] Trial 1 finished with value: 0.8658008575439453 and parameters: {'emb_size': 100.0, 'out_channels': 200.0, 'drop_rate': 0.30000000000000004, 'learning_rate': 0.024321959887221606, 'momentum': 0.5}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.9124  acc_train:0.6926  loss_dev:0.9094  acc_dev:0.6904\n",
            "epoch:2  loss_train:0.7160  acc_train:0.7487  loss_dev:0.7521  acc_dev:0.7390\n",
            "epoch:3  loss_train:0.5558  acc_train:0.8066  loss_dev:0.6493  acc_dev:0.7698\n",
            "epoch:4  loss_train:0.4260  acc_train:0.8654  loss_dev:0.5751  acc_dev:0.7925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:38:50,952] Trial 2 finished with value: 0.8103355169296265 and parameters: {'emb_size': 100.0, 'out_channels': 50.0, 'drop_rate': 0.5, 'learning_rate': 0.0011181016448577029, 'momentum': 0.9}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5  loss_train:0.3249  acc_train:0.9001  loss_dev:0.5223  acc_dev:0.8103\n",
            "epoch:1  loss_train:0.9429  acc_train:0.6839  loss_dev:0.9418  acc_dev:0.6866\n",
            "epoch:2  loss_train:0.7456  acc_train:0.7408  loss_dev:0.7752  acc_dev:0.7371\n",
            "epoch:3  loss_train:0.6000  acc_train:0.7851  loss_dev:0.6671  acc_dev:0.7769\n",
            "epoch:4  loss_train:0.4831  acc_train:0.8403  loss_dev:0.6023  acc_dev:0.7959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:39:33,908] Trial 3 finished with value: 0.8010687232017517 and parameters: {'emb_size': 300.0, 'out_channels': 50.0, 'drop_rate': 0.4, 'learning_rate': 0.0035535148932308424, 'momentum': 0.6}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5  loss_train:0.3934  acc_train:0.8737  loss_dev:0.5548  acc_dev:0.8011\n",
            "epoch:1  loss_train:0.6410  acc_train:0.7686  loss_dev:0.6953  acc_dev:0.7587\n",
            "epoch:2  loss_train:0.3653  acc_train:0.8959  loss_dev:0.5333  acc_dev:0.8149\n",
            "epoch:3  loss_train:0.1909  acc_train:0.9617  loss_dev:0.4432  acc_dev:0.8487\n",
            "epoch:4  loss_train:0.1051  acc_train:0.9929  loss_dev:0.4232  acc_dev:0.8528\n",
            "epoch:5  loss_train:0.0605  acc_train:0.9985  loss_dev:0.3939  acc_dev:0.8654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:40:32,143] Trial 4 finished with value: 0.8653950691223145 and parameters: {'emb_size': 300.0, 'out_channels': 150.0, 'drop_rate': 0.1, 'learning_rate': 0.0108909590298002, 'momentum': 0.5}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:1.0175  acc_train:0.6417  loss_dev:1.0111  acc_dev:0.6361\n",
            "epoch:2  loss_train:0.9086  acc_train:0.6833  loss_dev:0.9070  acc_dev:0.6906\n",
            "epoch:3  loss_train:0.8284  acc_train:0.7125  loss_dev:0.8407  acc_dev:0.7092\n",
            "epoch:4  loss_train:0.7561  acc_train:0.7349  loss_dev:0.7862  acc_dev:0.7204\n",
            "epoch:5  loss_train:0.6894  acc_train:0.7572  loss_dev:0.7385  acc_dev:0.7330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:41:22,139] Trial 5 finished with value: 0.7330222129821777 and parameters: {'emb_size': 300.0, 'out_channels': 100.0, 'drop_rate': 0.0, 'learning_rate': 0.001741461753940649, 'momentum': 0.5}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:1.0409  acc_train:0.6330  loss_dev:1.0239  acc_dev:0.6372\n",
            "epoch:2  loss_train:0.9368  acc_train:0.6764  loss_dev:0.9286  acc_dev:0.6770\n",
            "epoch:3  loss_train:0.8557  acc_train:0.7053  loss_dev:0.8597  acc_dev:0.7192\n",
            "epoch:4  loss_train:0.7882  acc_train:0.7246  loss_dev:0.8032  acc_dev:0.7286\n",
            "epoch:5  loss_train:0.7273  acc_train:0.7420  loss_dev:0.7617  acc_dev:0.7415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:42:24,125] Trial 6 finished with value: 0.7415449023246765 and parameters: {'emb_size': 400.0, 'out_channels': 150.0, 'drop_rate': 0.2, 'learning_rate': 0.0011764401518101388, 'momentum': 0.6}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.4183  acc_train:0.8524  loss_dev:0.5664  acc_dev:0.7999\n",
            "epoch:2  loss_train:0.1451  acc_train:0.9701  loss_dev:0.4482  acc_dev:0.8341\n",
            "epoch:3  loss_train:0.0499  acc_train:0.9978  loss_dev:0.4143  acc_dev:0.8494\n",
            "epoch:4  loss_train:0.0253  acc_train:0.9988  loss_dev:0.4130  acc_dev:0.8546\n",
            "epoch:5  loss_train:0.0182  acc_train:0.9987  loss_dev:0.4106  acc_dev:0.8598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:43:39,328] Trial 7 finished with value: 0.8597807884216309 and parameters: {'emb_size': 200.0, 'out_channels': 200.0, 'drop_rate': 0.0, 'learning_rate': 0.016379796306287792, 'momentum': 0.6}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.6259  acc_train:0.7824  loss_dev:0.6979  acc_dev:0.7446\n",
            "epoch:2  loss_train:0.3373  acc_train:0.9042  loss_dev:0.5280  acc_dev:0.8059\n",
            "epoch:3  loss_train:0.1817  acc_train:0.9615  loss_dev:0.4679  acc_dev:0.8308\n",
            "epoch:4  loss_train:0.0951  acc_train:0.9948  loss_dev:0.4327  acc_dev:0.8513\n",
            "epoch:5  loss_train:0.0569  acc_train:0.9983  loss_dev:0.4252  acc_dev:0.8431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:44:34,056] Trial 8 finished with value: 0.8430736064910889 and parameters: {'emb_size': 300.0, 'out_channels': 150.0, 'drop_rate': 0.1, 'learning_rate': 0.0045048889231495935, 'momentum': 0.8}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.7219  acc_train:0.7481  loss_dev:0.7448  acc_dev:0.7450\n",
            "epoch:2  loss_train:0.3331  acc_train:0.9143  loss_dev:0.5162  acc_dev:0.8182\n",
            "epoch:3  loss_train:0.1630  acc_train:0.9702  loss_dev:0.4725  acc_dev:0.8308\n",
            "epoch:4  loss_train:0.0805  acc_train:0.9897  loss_dev:0.4447  acc_dev:0.8568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:45:13,154] Trial 9 finished with value: 0.8460497856140137 and parameters: {'emb_size': 100.0, 'out_channels': 50.0, 'drop_rate': 0.5, 'learning_rate': 0.019164534949524418, 'momentum': 0.5}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5  loss_train:0.0415  acc_train:0.9982  loss_dev:0.4777  acc_dev:0.8460\n",
            "epoch:1  loss_train:nan  acc_train:0.4198  loss_dev:nan  acc_dev:0.4326\n",
            "epoch:2  loss_train:nan  acc_train:0.4201  loss_dev:nan  acc_dev:0.4326\n",
            "epoch:3  loss_train:nan  acc_train:0.4200  loss_dev:nan  acc_dev:0.4326\n",
            "epoch:4  loss_train:nan  acc_train:0.4199  loss_dev:nan  acc_dev:0.4326\n",
            "epoch:5  loss_train:nan  acc_train:0.4198  loss_dev:nan  acc_dev:0.4326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:46:00,623] Trial 10 finished with value: 0.4325622320175171 and parameters: {'emb_size': 400.0, 'out_channels': 100.0, 'drop_rate': 0.30000000000000004, 'learning_rate': 0.04489196420125491, 'momentum': 0.9}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.5141  acc_train:0.8460  loss_dev:0.6345  acc_dev:0.7721\n",
            "epoch:2  loss_train:0.1582  acc_train:0.9696  loss_dev:0.4378  acc_dev:0.8446\n",
            "epoch:3  loss_train:0.0519  acc_train:0.9975  loss_dev:0.4093  acc_dev:0.8553\n",
            "epoch:4  loss_train:0.0259  acc_train:0.9988  loss_dev:0.3931  acc_dev:0.8583\n",
            "epoch:5  loss_train:0.0194  acc_train:0.9987  loss_dev:0.4151  acc_dev:0.8531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:47:14,969] Trial 11 finished with value: 0.85308438539505 and parameters: {'emb_size': 200.0, 'out_channels': 200.0, 'drop_rate': 0.30000000000000004, 'learning_rate': 0.008590682908038879, 'momentum': 0.8}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:1.0931  acc_train:0.5855  loss_dev:1.0715  acc_dev:0.6097\n",
            "epoch:2  loss_train:1.0227  acc_train:0.6395  loss_dev:1.0030  acc_dev:0.6606\n",
            "epoch:3  loss_train:0.9582  acc_train:0.6715  loss_dev:0.9425  acc_dev:0.6877\n",
            "epoch:4  loss_train:0.9041  acc_train:0.6895  loss_dev:0.8938  acc_dev:0.7026\n",
            "epoch:5  loss_train:0.8590  acc_train:0.7070  loss_dev:0.8538  acc_dev:0.7256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:48:09,917] Trial 12 finished with value: 0.725581705570221 and parameters: {'emb_size': 200.0, 'out_channels': 100.0, 'drop_rate': 0.4, 'learning_rate': 0.0005265940138148248, 'momentum': 0.7}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:88.8337  acc_train:0.6163  loss_dev:80.8621  acc_dev:0.6375\n",
            "epoch:2  loss_train:37.8870  acc_train:0.7984  loss_dev:58.4575  acc_dev:0.7257\n",
            "epoch:3  loss_train:17.7868  acc_train:0.9080  loss_dev:44.5391  acc_dev:0.8364\n",
            "epoch:4  loss_train:7.1672  acc_train:0.9621  loss_dev:43.0792  acc_dev:0.8617\n",
            "epoch:5  loss_train:6.8836  acc_train:0.9690  loss_dev:53.3706  acc_dev:0.8554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:49:52,330] Trial 13 finished with value: 0.8553841710090637 and parameters: {'emb_size': 100.0, 'out_channels': 200.0, 'drop_rate': 0.4, 'learning_rate': 0.0453975264082803, 'momentum': 0.8}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.6436  acc_train:0.7696  loss_dev:0.7044  acc_dev:0.7468\n",
            "epoch:2  loss_train:0.3465  acc_train:0.9303  loss_dev:0.5520  acc_dev:0.8238\n",
            "epoch:3  loss_train:0.1723  acc_train:0.9754  loss_dev:0.4568  acc_dev:0.8405\n",
            "epoch:4  loss_train:0.0878  acc_train:0.9950  loss_dev:0.4189  acc_dev:0.8590\n",
            "epoch:5  loss_train:0.0497  acc_train:0.9988  loss_dev:0.4073  acc_dev:0.8665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:50:58,406] Trial 14 finished with value: 0.8664772510528564 and parameters: {'emb_size': 200.0, 'out_channels': 150.0, 'drop_rate': 0.2, 'learning_rate': 0.007345041174066163, 'momentum': 0.7}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.6709  acc_train:0.7570  loss_dev:0.7231  acc_dev:0.7383\n",
            "epoch:2  loss_train:0.3777  acc_train:0.8809  loss_dev:0.5571  acc_dev:0.7940\n",
            "epoch:3  loss_train:0.2023  acc_train:0.9644  loss_dev:0.4742  acc_dev:0.8350\n",
            "epoch:4  loss_train:0.1136  acc_train:0.9901  loss_dev:0.4546  acc_dev:0.8364\n",
            "epoch:5  loss_train:0.0660  acc_train:0.9975  loss_dev:0.4426  acc_dev:0.8456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:51:53,646] Trial 15 finished with value: 0.8456438779830933 and parameters: {'emb_size': 200.0, 'out_channels': 100.0, 'drop_rate': 0.2, 'learning_rate': 0.006814857305992061, 'momentum': 0.7}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.7183  acc_train:0.7412  loss_dev:0.7458  acc_dev:0.7309\n",
            "epoch:2  loss_train:0.4442  acc_train:0.8408  loss_dev:0.5713  acc_dev:0.8011\n",
            "epoch:3  loss_train:0.2578  acc_train:0.9387  loss_dev:0.4841  acc_dev:0.8341\n",
            "epoch:4  loss_train:0.1533  acc_train:0.9814  loss_dev:0.4445  acc_dev:0.8453\n",
            "epoch:5  loss_train:0.0910  acc_train:0.9959  loss_dev:0.4277  acc_dev:0.8501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:52:54,782] Trial 16 finished with value: 0.8501082062721252 and parameters: {'emb_size': 400.0, 'out_channels': 150.0, 'drop_rate': 0.2, 'learning_rate': 0.0037104881058191725, 'momentum': 0.8}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.3916  acc_train:0.8724  loss_dev:0.5380  acc_dev:0.8038\n",
            "epoch:2  loss_train:0.1296  acc_train:0.9774  loss_dev:0.4323  acc_dev:0.8494\n",
            "epoch:3  loss_train:0.0436  acc_train:0.9968  loss_dev:0.4283  acc_dev:0.8509\n",
            "epoch:4  loss_train:0.0223  acc_train:0.9989  loss_dev:0.4182  acc_dev:0.8657\n",
            "epoch:5  loss_train:0.0166  acc_train:0.9989  loss_dev:0.4281  acc_dev:0.8661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:53:42,150] Trial 17 finished with value: 0.8661391139030457 and parameters: {'emb_size': 300.0, 'out_channels': 100.0, 'drop_rate': 0.1, 'learning_rate': 0.005630747746703537, 'momentum': 0.9}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.6406  acc_train:0.7536  loss_dev:0.6918  acc_dev:0.7415\n",
            "epoch:2  loss_train:0.2350  acc_train:0.9494  loss_dev:0.4621  acc_dev:0.8356\n",
            "epoch:3  loss_train:0.0967  acc_train:0.9941  loss_dev:0.4364  acc_dev:0.8446\n",
            "epoch:4  loss_train:0.0377  acc_train:0.9987  loss_dev:0.4053  acc_dev:0.8568\n",
            "epoch:5  loss_train:0.0238  acc_train:0.9988  loss_dev:0.4180  acc_dev:0.8516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:54:42,389] Trial 18 finished with value: 0.8515962958335876 and parameters: {'emb_size': 200.0, 'out_channels': 150.0, 'drop_rate': 0.4, 'learning_rate': 0.011597151999298564, 'momentum': 0.7}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.7381  acc_train:0.7459  loss_dev:0.7598  acc_dev:0.7365\n",
            "epoch:2  loss_train:0.3290  acc_train:0.8954  loss_dev:0.5076  acc_dev:0.8197\n",
            "epoch:3  loss_train:0.1493  acc_train:0.9709  loss_dev:0.4386  acc_dev:0.8420\n",
            "epoch:4  loss_train:0.0686  acc_train:0.9962  loss_dev:0.4322  acc_dev:0.8509\n",
            "epoch:5  loss_train:0.0367  acc_train:0.9990  loss_dev:0.4248  acc_dev:0.8572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:55:30,162] Trial 19 finished with value: 0.8572105169296265 and parameters: {'emb_size': 300.0, 'out_channels': 100.0, 'drop_rate': 0.5, 'learning_rate': 0.006574549387806996, 'momentum': 0.8}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1  loss_train:0.9531  acc_train:0.6748  loss_dev:0.9472  acc_dev:0.6839\n",
            "epoch:2  loss_train:0.7837  acc_train:0.7242  loss_dev:0.8057  acc_dev:0.7167\n",
            "epoch:3  loss_train:0.6539  acc_train:0.7745  loss_dev:0.7114  acc_dev:0.7505\n",
            "epoch:4  loss_train:0.5414  acc_train:0.8044  loss_dev:0.6311  acc_dev:0.7680\n",
            "epoch:5  loss_train:0.4431  acc_train:0.8612  loss_dev:0.5773  acc_dev:0.7978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-22 10:56:33,342] Trial 20 finished with value: 0.7977544069290161 and parameters: {'emb_size': 200.0, 'out_channels': 150.0, 'drop_rate': 0.30000000000000004, 'learning_rate': 0.0026250992718907813, 'momentum': 0.6}. Best is trial 0 with value: 0.8683712482452393.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "  Value: 0.868\n",
            "  Params: \n",
            "    emb_size: 300.0\n",
            "    out_channels: 100.0\n",
            "    drop_rate: 0.4\n",
            "    learning_rate: 0.006774270419053273\n",
            "    momentum: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir nlp88\n",
        "n_input = len(set(word2id.values())) + 1\n",
        "n_embed = int(trial.params['emb_size'])  # 単語ベクトルの次元\n",
        "n_output = 4  # 出力ベクトルの次元（=ラベルの種類数）\n",
        "padding_idx = len(set(word2id.values()))\n",
        "out_channels = int(trial.params['out_channels'])\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1\n",
        "learning_rate = trial.params['learning_rate']\n",
        "momentum = trial.params['momentum']\n",
        "drop_rate = trial.params['drop_rate']\n",
        "nlp = \"nlp88\"\n",
        "\n",
        "model = CNN(n_input, n_embed, n_output, padding_idx, out_channels, kernel_size, stride, padding, drop_rate)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #損失関数\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) # 最適化手法\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_model(32, model, criterion, optimizer, 10, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BoVNUH_wf6q",
        "outputId": "56e92b8a-4f40-42c7-b2cb-f2859c1fc8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘nlp88’: File exists\n",
            "epoch:1  loss_train:0.5239  acc_train:0.8165  loss_dev:0.6162  acc_dev:0.7792\n",
            "epoch:2  loss_train:0.1516  acc_train:0.9618  loss_dev:0.4699  acc_dev:0.8341\n",
            "epoch:3  loss_train:0.0560  acc_train:0.9937  loss_dev:0.4825  acc_dev:0.8494\n",
            "epoch:4  loss_train:0.0219  acc_train:0.9989  loss_dev:0.4856  acc_dev:0.8519\n",
            "epoch:5  loss_train:0.0172  acc_train:0.9986  loss_dev:0.4796  acc_dev:0.8586\n",
            "epoch:6  loss_train:0.0202  acc_train:0.9978  loss_dev:0.5235  acc_dev:0.8504\n",
            "epoch:7  loss_train:0.0131  acc_train:0.9990  loss_dev:0.5040  acc_dev:0.8512\n",
            "epoch:8  loss_train:0.0153  acc_train:0.9989  loss_dev:0.4995  acc_dev:0.8653\n",
            "epoch:9  loss_train:0.0123  acc_train:0.9990  loss_dev:0.5117  acc_dev:0.8631\n",
            "epoch:10  loss_train:0.0129  acc_train:0.9990  loss_dev:0.5121  acc_dev:0.8542\n"
          ]
        }
      ]
    }
  ]
}